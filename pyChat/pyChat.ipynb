{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 77 kB/s eta 0:00:017\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in ./venv/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in ./venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python3.7 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en') #Loads the spacy en model into a python object\n",
    "doc = nlp(u'I am learning how to build chatbots') #Creates a doc object\n",
    "for token in doc: \n",
    "    print(token.text, token.pos_) #prints the text and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "going VERB\n",
      "to ADP\n",
      "London PROPN\n",
      "next ADJ\n",
      "week NOUN\n",
      "for ADP\n",
      "a DET\n",
      "meeting NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am going to London next week for a meeting.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Google PROPN NNP compound Xxxxx True False\n",
      "release release VERB VB nmod xxxx True False\n",
      "\" \" PUNCT `` punct \" False False\n",
      "Move Move PROPN NNP nmod Xxxx True True\n",
      "Mirror Mirror PROPN NNP nmod Xxxxx True False\n",
      "\" \" PUNCT '' punct \" False False\n",
      "AI AI PROPN NNP compound XX True False\n",
      "experiment experiment NOUN NN ROOT xxxx True False\n",
      "that that DET WDT nsubj xxxx True True\n",
      "matches match VERB VBZ relcl xxxx True False\n",
      "your -PRON- DET PRP$ poss xxxx True True\n",
      "pose pose NOUN NN dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "80,000 80,000 NUM CD nummod dd,ddd False False\n",
      "images image NOUN NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "for token in doc:\n",
    "          print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj X True True\n",
      "am be AUX VBP aux xx True True\n",
      "learning learn VERB VBG ROOT xxxx True False\n",
      "how how ADV WRB advmod xxx True True\n",
      "to to PART TO aux xx True True\n",
      "build build VERB VB xcomp xxxx True False\n",
      "chatbots chatbot NOUN NNS dobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastest']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "lemmatizer = English.Defaults.create_lemmatizer()\n",
    "lemmatizer('chuckles', 'NOUN')\n",
    "lemmatizer('fastest', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively\n",
    "#import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#lemmatizer = nlp.Defaults.create_lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.7/site-packages (from nltk) (4.55.0)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.7/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.7/site-packages (from nltk) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(porter_stemmer.stem(\"fastest\"))\n",
    "print(snowball_stemmer.stem(\"fastest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109.65 billion US dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14, 1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO.\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:00 AM TIME\n",
      "90% PERCENT\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string1 = u\"Imagine Dragons are the best band.\"\n",
    "my_string2 = u\"Imagine dragons come and take over the city.\"\n",
    "doc1 = nlp(my_string1)\n",
    "doc2 = nlp(my_string2)\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sometime', 'were', 'back', 'whereas', 'may', 'thence', 'thereby', 'take', 'wherever', 'she', 'your', 'amount', 'been', 'has', 'fifteen', 'does', 'few', 'twenty', 'hereafter', 'there', 'hers', 'less', 'between', '’d', 'once', 'an', 'first', 'both', 'five', 'if', 'forty', '’ll', 'onto', 'yours', 'our', 'about', 'us', 'except', 'somewhere', '‘re', 'since', 'cannot', 'her', '’re', 'elsewhere', 'what', 'show', 'on', 'enough', 'various', 'below', \"'s\", 'had', 'than', 'eight', 'somehow', 'call', 'sixty', 'four', 'even', 'say', 're', 'or', 'please', 'via', '‘m', 'are', 'itself', 'quite', 'twelve', 'nor', 'seemed', 'several', 'whither', 'why', 'used', 'together', 'ever', 'full', 'everywhere', 'against', 'his', 'bottom', 'should', 'either', 'latterly', 'some', 'be', 'whose', 'whence', 'off', 'for', 'each', 'have', 'keep', 'nobody', 'hereupon', 'thru', 'seem', 'others', 'per', 'is', 'hundred', 'out', 'where', 'hereby', 'least', 'of', 'though', 'therefore', 'everything', 'alone', 'before', \"'m\", 'but', 'can', 'third', 'indeed', '’ve', 'down', 'such', 'these', 'yourselves', 'yet', 'ca', 'meanwhile', 'being', \"'re\", 'towards', 'herself', 'at', 'become', 'go', 'might', 'doing', 'most', 'besides', 'until', 'whenever', 'along', 'noone', 'who', 'something', '‘ve', 'many', 'nothing', 'perhaps', 'across', 'own', 'which', \"'ll\", 'during', 'two', 'and', 'three', \"'d\", 'nevertheless', 'six', 'herein', 'much', '‘ll', 'around', 'could', 'behind', 'all', 'amongst', 'here', 'n’t', 'sometimes', 'top', 'namely', 'whereby', 'into', 'its', 'beforehand', 'mostly', 'he', 'moreover', 'someone', 'give', 'every', 'afterwards', 'none', 'whatever', 'whereafter', 'n‘t', \"n't\", 'part', 'thereafter', 'no', 'put', 'former', 'him', 'really', 'anywhere', 'serious', 'was', 'through', 'very', 'up', 'same', 'formerly', 'so', 'empty', 'already', 'neither', 'beyond', 'front', 'nine', 'am', 'the', 'next', 'will', 'seeming', 'not', 'in', 'would', 'also', 'among', 'well', 'get', 'one', \"'ve\", 'as', 'themselves', 'they', 'name', 'make', 'do', 'after', 'whom', 'ten', 'therein', 'again', 'although', '‘s', 'any', 'done', 'that', 'see', 'anyone', 'latter', 'thereupon', 'while', 'by', 'seems', 'how', 'toward', '’m', 'nowhere', 'without', 'ours', 'last', 'everyone', 'becoming', 'myself', 'wherein', 'never', 'above', 'ourselves', 'side', 'did', 'over', 'you', 'i', 'beside', 'anyhow', 'other', 'eleven', 'those', 'whoever', 'always', 'this', 'mine', 'still', 'when', '‘d', 'often', 'rather', 'whole', 'becomes', 'with', 'move', 'else', 'made', 'hence', 'under', 'it', 'more', 'because', 'must', 'only', 'me', 'just', 'thus', 'within', 'using', '’s', 'their', 'otherwise', 'yourself', 'further', 'my', 'fifty', 'upon', 'too', 'regarding', 'became', 'whether', 'whereupon', 'himself', 'throughout', 'we', 'then', 'them', 'due', 'however', 'to', 'anyway', 'almost', 'a', 'another', 'now', 'from', 'anything', 'unless'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Japan to France')\n",
    "jpn, fra = doc[5], doc[7]\n",
    "list(fra.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[flight, Book]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[2].ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of ferry belongs to island\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Book a movie at the cinema and the ferry to the island')\n",
    "tasks = doc[2], doc[8] #(movie, ferry)\n",
    "tasks_target = doc[5], doc[11] #(cinema, island)\n",
    "\n",
    "for task in tasks_target: \n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok,task))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cinema]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[3].children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"252ae655979d47299195cc262f7657db-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Book</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">table</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">restaurant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">taxi</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">hotel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-0\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,354.0 L578.0,342.0 562.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-7\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-8\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 1625.0,2.0 1625.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,354.0 L1633.0,342.0 1617.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-252ae655979d47299195cc262f7657db-0-10\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-252ae655979d47299195cc262f7657db-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is asking: Paris to visit\n",
      "User is asking: Montreal to stay\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'What are some places to visit in Paris and stay in Montreal?')\n",
    "\n",
    "places = [doc[7], doc[11]]\n",
    "actions = [doc[5],doc[9]]\n",
    "\n",
    "for place in places:\n",
    "    for tok in place.ancestors:\n",
    "        if tok in actions:\n",
    "            print(\"User is asking: {} to {}\".format(place, tok))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tiger lions, merchandise, %]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Tiger lions are producing merchandise at %50 off')\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning learning nsubj cracks\n",
      "the code code dobj cracks\n",
      "messenger RNAs RNAs pobj of\n",
      "protein-coding potential potential conj RNAs\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Deep learning cracks the code of messenger RNAs and protein-coding potential')\n",
    "for chunk in doc.noun_chunks: \n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [ 0.6124835   1.4812975  -0.72636694  0.06934005  4.5061274 ]\n",
      "are [-3.683414   1.6739068  0.8058587 -1.5030262 -1.456842 ]\n",
      "you [-1.982389   0.1802355 -1.5821393 -0.9292287  3.5329905]\n",
      "doing [ 1.4059318  -3.2353983  -0.499872   -1.289648   -0.42563045]\n",
      "today [ 1.0131264 -3.218736  -2.291664  -1.326089  -0.6509166]\n",
      "? [ 1.5639621  -1.0553905  -1.8733073  -1.7224176  -0.41755062]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'How are you doing today?')\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2962460212582995\n",
      "0.6818258855051886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "hello_doc = nlp(u'hello')\n",
    "hi_doc = nlp(u'hi')\n",
    "yo_doc = nlp(u'yo')\n",
    "print(hi_doc.similarity(yo_doc))\n",
    "print(hello_doc.similarity(hi_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 51% similar to word truck\n",
      "Word car is 17% similar to word google\n",
      "Word truck is 51% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 36% similar to word google\n",
      "Word google is 17% similar to word car\n",
      "Word google is 36% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "example_doc = nlp(u'car truck google')\n",
    "\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print(\"Word {} is {}% similar to word {}\".format(t1.text, similarity_perc, t2.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "the\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Brexit is the impending withdrawal of the U.K. from the European Union.')\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "#add_special_case : can be used to add your own rule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
