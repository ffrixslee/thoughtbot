{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 42 kB/s eta 0:00:019    |█████████████████████▉          | 8.2 MB 20 kB/s eta 0:03:03     |███████████████████████████████▋| 11.9 MB 60 kB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in ./venv/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in ./venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python3.7 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en') #Loads the spacy en model into a python object\n",
    "doc = nlp(u'I am learning how to build chatbots') #Creates a doc object\n",
    "for token in doc: \n",
    "    print(token.text, token.pos_) #prints the text and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "going VERB\n",
      "to ADP\n",
      "London PROPN\n",
      "next ADJ\n",
      "week NOUN\n",
      "for ADP\n",
      "a DET\n",
      "meeting NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am going to London next week for a meeting.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Google PROPN NNP compound Xxxxx True False\n",
      "release release VERB VB nmod xxxx True False\n",
      "\" \" PUNCT `` punct \" False False\n",
      "Move Move PROPN NNP nmod Xxxx True True\n",
      "Mirror Mirror PROPN NNP nmod Xxxxx True False\n",
      "\" \" PUNCT '' punct \" False False\n",
      "AI AI PROPN NNP compound XX True False\n",
      "experiment experiment NOUN NN ROOT xxxx True False\n",
      "that that DET WDT nsubj xxxx True True\n",
      "matches match VERB VBZ relcl xxxx True False\n",
      "your -PRON- DET PRP$ poss xxxx True True\n",
      "pose pose NOUN NN dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "80,000 80,000 NUM CD nummod dd,ddd False False\n",
      "images image NOUN NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "for token in doc:\n",
    "          print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj X True True\n",
      "am be AUX VBP aux xx True True\n",
      "learning learn VERB VBG ROOT xxxx True False\n",
      "how how ADV WRB advmod xxx True True\n",
      "to to PART TO aux xx True True\n",
      "build build VERB VB xcomp xxxx True False\n",
      "chatbots chatbot NOUN NNS dobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' (/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/spacy/lang/en/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e8582315d7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLEMMA_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_EXC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_RULES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEMMA_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_EXC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_RULES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chuckles'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2nd param is token's part-of-speech tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' (/Users/nurliyana/thoughtbot/pyChat/venv/lib/python3.7/site-packages/spacy/lang/en/__init__.py)"
     ]
    }
   ],
   "source": [
    "#outdated! Refer to: https://stackoverflow.com/questions/58779371/importerror-cannot-import-name-lemma-index-from-spacy-lang-en\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "lemmatizer('chuckles', 'NOUN') # 2nd param is token's part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastest']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "lemmatizer = English.Defaults.create_lemmatizer()\n",
    "lemmatizer('chuckles', 'NOUN')\n",
    "lemmatizer('fastest', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively\n",
    "#import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#lemmatizer = nlp.Defaults.create_lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a5ee00e168b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mporter_stemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msnowball_stemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fastest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 414 kB/s eta 0:00:01    |█                               | 40 kB 373 kB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./venv/lib/python3.7/site-packages (from nltk) (4.55.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 425 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 193 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 164 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=8bf3bb8c62e697b4430a51b4598db6a3d99dcb01651b903bc395f371888b3213\n",
      "  Stored in directory: /Users/nurliyana/Library/Caches/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-7.1.2 joblib-1.0.0 nltk-3.5 regex-2020.11.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(porter_stemmer.stem(\"fastest\"))\n",
    "print(snowball_stemmer.stem(\"fastest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109.65 billion US dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14, 1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO.\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:00 AM TIME\n",
      "90% PERCENT\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string1 = u\"Imagine Dragons are the best band.\"\n",
    "my_string2 = u\"Imagine dragons come and take over the city.\"\n",
    "doc1 = nlp(my_string1)\n",
    "doc2 = nlp(my_string2)\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
